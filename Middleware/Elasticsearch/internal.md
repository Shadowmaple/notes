# Elasticsearch 原理

Elasticsearch 基于 Lucene 搜索引擎，Elasticsearch 在此基础上做了高可用、高扩展、高性能。

## Lucene

Lucene 是一个**单机文本检索库**，具备完整的单机搜索引擎功能。但不具备高性能、高扩展性、高可用。

### Segment

segment 是 Lucene 的具备**完整搜索功能的最小单元**

segment 的组成部分：

- Inverted Index 倒排索引：用于搜索
- Term Index：用于加速搜索
- Stored Fields：存放文档的原始信息
- Doc Values：用于排序和聚合

![Segment是什么](https://cdn.xiaobaidebug.top/1719060773326.jpeg)

### 倒排索引

将文档根据单词、短语进行分词，每个词项指向对应的多个文档。

这些单词又按照字典序排列，方便二分搜索。

- Term Dictionary：排好序的词项索引
- Posting List：文档ID集合、词频、词项在文本里的偏移量等信息

Term Dictionary 数据量很大，放**内存**并不现实，因此必须放在**磁盘**中。

### Term Index

将 Term 的共同前缀提取出来，就组成了一棵前缀字典树，叶子节点存放这些词项在磁盘中的偏移量，即一个精简的目录树。

![Term Index是什么](https://cdn.xiaobaidebug.top/1719060664024.jpeg)

Term Index 的作用：

- 使用更少的空间容纳全部的 Term Dictionary，方便存放于内存，快速查找

### Doc Values

为了快速处理排序、聚合，文档的所有字段被统一、集中起来，形成一个列式存储。对某个字段排序的时候，就只需要将这些集中存放的字段一次性读取出来，就能做到针对性地进行排序。

![Doc Values是什么](https://cdn.xiaobaidebug.top/1719060757527.jpeg)

### 请求处理

为尽可能提高并发读写的效率，Segment 一旦生成就不能被修改，如果新增文档，就生成新的文档。这样读、写不会影响。

当 segment 数量过多时，会触发**段合并**(segment merging)。不定期合并多个小 segment，变成一个大 segment。

segment 之间并不会再生成一个搜索目录，当读请求到来时，不知道具体需要访问哪个 segment，因此会**并发同时读**多个segment。

## 集群

Elasticsearch 在 Lucene 的基础上，定义了一系列的角色：

- 分片 Shard：每个分片底层是一个独立的 Lucene 库
  - 主分片 Primary shard：写操作
  - 副本分片 Replica shard：读操作；主分片失效时选举成为主分片
- Index：由1个或多个分片组成，外部请求的单元
- Node：一个Node就是一个Elasticsearch实例，包含一个或多个分片

Node 使得 ES 能够分布式、水平扩展

多节点、主副分片保障高可用

Index、主副分片、多节点保障读写的高性能

### Node 角色

- Master Node：负责管理集群，添加、删除节点。只有候选主节点的节点才能成为主节点。
- Data Node：存储数据、执行数据请求
- Coordinate Node 协调节点：
  - 负责接收请求、分发任务、汇总结果。将集群请求转发到主节点，将与数据相关的请求转发到数据节点。
  - **任何节点都可以作为协调节点**（除非通过配置限制），不存在一个 “专门的协调节点” 等待其他节点转发请求。
- Ingest Node：用于在索引之前对文档进行预处理，比如字段解析、时间格式转换、字段增删等（如有Logstash则不是必要的）。搜索时不会用到。

集群 Node 使用**类似一致性算法 Raft** 来选举主节点、探测Node状态

### 主节点选举

只有主候选节点才能参与主节点的选举、投票

```yaml
node.roles: [ master ]  # 或旧写法：node.master: true
```

主节点选举在以下情况被触发：

1. 集群首次启动。
2. 当前主节点宕机或失联。
3. 集群中多数主候选节点无法与主节点通信（触发故障检测）。
4. 新节点加入并发现无主状态。

核心机制：基于投票的共识算法（类 Raft 思想，基于 Zen Discovery 的分布式共识协议）

选举过程：

1. **发现阶段 Discovery**
   1. 每个节点启动时，通过配置的 `discovery.seed_hosts`（替代旧版 `unicast.hosts`）尝试连接已知的主候选节点。
   2. 使用 **gossip 协议** 交换集群状态信息。
   3. 节点通过 `cluster.initial_master_nodes` 配置来引导首次选举（仅在首次启动时需要）。
2. **候选节点发起投票请求（Candidate State）**
   - 当一个主候选节点发现当前无主（no master），且自己已连接到足够多的主候选节点时，它会进入“候选状态”。
   - 它会向其他主候选节点发送投票请求（Vote Request），包含：
     - 自己的节点 ID
     - 当前集群版本（cluster state version）
     - 任期（term，递增的选举轮次编号）
3. **投票规则（Voting）**。每个主候选节点根据以下规则决定是否投票：
   - 同一任期内，每个节点只能投一票。
   - 投票给 **最先请求且自身认为“最有资格”的节点**。
   - “资格”判断依据：
     - 节点是否在 `cluster.initial_master_nodes` 列表中（仅首次选举）。
     - 节点是否健康、可通信。
     - 集群状态是否较新（避免投给状态过旧的节点）。
   - 必须获得 **多数票（majority）** 才能成为主节点。

4. **赢得选举（Election Winner）**
   - 当某个候选节点收到 **多数主候选节点的投票**，它就成为新的主节点。
   - 它会立即发布一个 **新的集群状态（Cluster State）**，宣告自己为主节点，并开始协调集群操作（如分片分配、索引创建等）。
5. **集群状态同步**
   - 新主节点负责维护和广播最新的集群状态（Cluster State）。
   - 其他节点定期向主节点汇报状态，主节点通过 **最小集合广播（diff-based publishing）** 将变更推送给所有节点。

### 写请求

处理写请求：

1. 协调节点根据 hash 路由，判断数据该写入到哪个数据节点里的哪个分片，找到主分片并发送写入消息
2. 主分片处理写请求，内部Lucene生成新的segment，将数据固化为倒排索引和 Stored Fields 以及 Doc Values 等多种结构
3. 主分片向副本分片发送数据同步的复制消息
4. 副本分片复制成功后返回ACK
5. 主分片接收到指定数量（策略配置）的副本分片ACK后，返回给协同节点ACK
6. 协调节点响应客户端应用写入完成

路由算法：

根据路由或文档 id 计算目标的分片 id。

```txt
shard = hash(_routing) % (num_of_primary_shards)
shard = hash(document_id) % (num_of_primary_shards)
```

### 读请求

查询流程分为 “查询（Query）” 和 “取回（Fetch）” 两个阶段

**Query Phase** 查询阶段

任务：根据搜索请求找到文档ID并进行一次排序聚合，找到最终需要返回的文档的ID

流程：

1. 协调节点根据 index 了解到分片数量及分布节点位置，一般通过轮询所有的主副分片来达到分片的负载均衡，将请求转发到这些数据节点的分片上面；
2. 搜索请求到达分片后，分片底层的 lucene 库会**并发**搜索多个 segment，利用每个 segment 内部的**倒排索引**获取到对应**文档 id**，并结合 **doc values** 获得**排序信息**。分片将结果聚合返回给**协调节点**。
3. **协调节点**对多个分片中拿到的数据进行**一次排序聚合**，**舍弃**大部分不需要的数据。

**Fetch Phase** 获取阶段

任务：根据文档ID并行获取完整的文档数据

流程：

1. **协调节点**根据**文档 ID** 请求**数据节点**里的 **分片**，分片底层的 lucene 库会从 segment 内的 **Stored Fields** 中取出**完整文档内容**，并返回给协调节点。（并行/并发？）
2. **协调节点**最终将数据结果返回给**客户端**。完成整个搜索过程。

在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。

### Translog（事务日志）

核心作用

1. **数据可靠性保障**当文档被索引（`index`/`create`/`update`/`delete`）时，ES 会先将操作记录写入 Translog，再更新内存中的倒排索引（`in-memory buffer`）。即使节点在数据刷新到磁盘前宕机，重启后 ES 可通过 Translog 恢复未持久化的操作，避免数据丢失。
2. **支持 “实时 CRUD”**文档写入后，无需等待数据刷盘即可被查询（内存索引可见），而 Translog 确保了这些 “未刷盘数据” 的安全性，兼顾了查询实时性与数据可靠性。
3. **分片恢复依据**当分片（主分片或副本分片）需要从其他节点恢复时，Translog 是增量同步的核心依据 —— 主分片会将 Translog 中未同步的操作发送给副本分片，确保数据一致。

Translog 的工作流程

1. **写入阶段**
   - 客户端发送索引请求，文档先被写入 **内存缓冲区（in-memory buffer）**。
   - 同时，操作记录（如 “新增文档 ID=1”“删除文档 ID=2”）被追加到 **Translog**（先写内存中的 Translog 缓冲区，再定期刷到磁盘，在7.x中默认立刻刷新到磁盘）。
2. **刷新（Refresh）阶段**
   - **默认每 1 秒**（可通过 `index.refresh_interval` 调整），内存缓冲区中的数据会被写入新的 **segment**（暂存于操作系统缓存`Filesystem Cache`中），形成可查询的倒排索引。
   - **目的：让新数据尽快可查询（近实时性）**，属于高频、轻量。此时数据已可被搜索，但尚未完全持久化（segment仍在操作系统缓存中），Translog 仍需保留这些操作记录，避免宕机造成写数据丢失。
3. **刷写（Flush）阶段**
   - 当满足以下条件时，ES 会触发Flush操作（将数据彻底持久化到磁盘）：
     - Translog 大小达到阈值（默认 512MB，由 `index.translog.flush_threshold_size` 控制）。
     - 超过一定时间未刷写（默认 30 分钟，由 `index.translog.flush_threshold_period` 控制）。
     - 手动执行 `POST /index/_flush` 命令。
   - Flush 过程：
     1. 将内存缓冲区中剩余数据刷新为新的段文件。
     2. 强制操作系统将所有segment从缓存刷到磁盘（通过 `fsync`）。
     3. 清空并滚动 Translog（生成新的 Translog 文件，旧文件可删除或归档）。
   - **目的：确保所有数据彻底持久化到磁盘（可靠性）**，属于低频、重量。

Translog 的关键特性与配置

1. **持久化策略**
   - Translog 本身采用 “追加写入” 模式，且会定期通过 `fsync` 刷到磁盘（确保断电不丢失）。
   - 7.x 中默认配置为 `index.translog.durability: request`，即**每次请求完成后立即刷 Translog 到磁盘**（最高可靠性，性能略有损耗）。“每次请求完成后”具体是指写入阶段完成、返回客户端之前，只有`fsync`执行成功后才会返回客户端。
   - 若允许短暂数据丢失以换取性能，可改为 `async`（每隔 5 秒自动刷盘）。
2. **文件管理**
   - Translog 由多个文件组成，每次 Flush 后会生成新文件，旧文件在确认所有数据已持久化后删除。
   - 可通过 `index.translog.retention.size` 和 `index.translog.retention.age` 控制历史 Translog 文件的保留策略（用于分片恢复）。
3. **与段文件的关系**
   - 段文件是最终的倒排索引存储（持久化后不可修改），Translog 是临时的操作日志（可追加、可删除）。
   - 段文件的合并（Merge）操作不会影响 Translog，因为合并仅优化段结构，不改变数据本身。

举例：Translog 如何避免数据丢失？

假设发生以下场景：

1. 客户端写入文档 A，ES 将其存入内存缓冲区，并记录到 Translog。
2. 1 秒后，内存缓冲区刷新为段文件（文档 A 可查询），但段文件仍在操作系统缓存中（未真正写入磁盘）。
3. 此时节点突然宕机，内存数据和操作系统缓存中的段文件丢失。
4. 节点重启后，ES 会读取 Translog，发现 “文档 A 已写入但未持久化”，重新执行该操作，将文档 A 恢复到新的段文件中。

总结

Translog 是 ES 数据可靠性的 “安全网”，其核心价值在于：

- 确保所有已确认的索引操作不会因节点故障丢失。
- 允许 ES 采用 “先内存后磁盘” 的架构，平衡实时性与性能。
- 是分片同步和故障恢复的关键依据。

## 参考

- Elasticsearch官方文档
- Elasticsearch运维开发
- [elasticSearch 是什么？工作原理是怎么样的？](https://golangguide.top/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9/elasticSearch%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.htm)
